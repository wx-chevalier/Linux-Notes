# 计算机系统架构

# CPU

简而言之，计算机由连接到内存的中央处理器（CPU）组成。下图说明了所有计算机操作背后的一般原理。

![The CPU 原理简化](https://s2.ax1x.com/2020/01/26/1nPs54.png)

CPU 对寄存器中保存的值执行指令。此示例显示首先将 R1 的值设置为 100，将值从存储器位置 0x100 加载到 R2，将这两个值加在一起并将结果放入 R3，最后将新值（110）存储到 R4 以供进一步使用。CPU 执行从内存读取的指令，指令有两大类：

- 那些将值从存储器加载到寄存器，并将值从寄存器存储到存储器的函数。

- 那些对存储在寄存器中的值进行运算的变量。例如，对两个寄存器中的值进行相加，相减或相除，执行按位运算（和，或，异或等）或执行其他数学运算（平方根，sin，cos，tan 等）。

在上面的示例中，我们仅将 100 加到存储在内存中的值，然后将此新结果存储回内存。

## 指令基础

### Branching

除了加载或存储，CPU 的另一个重要操作是分支。在内部，CPU 在指令指针中保留要执行的下一条指令的记录。通常，指令指针递增以顺序指向下一条指令。分支指令通常将检查特定寄存器是否为零或是否设置了标志，如果是，则将指针修改为另一个地址。因此，下一条要执行的指令将来自程序的不同部分。这就是循环和决策语句的工作方式。

例如，可以通过查找两个寄存器中的或来实现类似 if（x == 0）的语句，其中一个保存 x，另一个保存零。如果结果为零，则比较为真（即 x 的所有位均为零），并且应采用语句的主体，否则分支通过主体代码。

### Cycles

我们都熟悉以兆赫兹或千兆赫兹（每秒数百万或数亿个周期）给出的计算机速度。之所以称为时钟速度，是因为它是计算机内部时钟的脉动速度。

在处理器内使用脉冲以保持其内部同步。在每个滴答声或脉冲时，可以开始另一种操作； 就像时钟拍打鼓的人一样，使划船者的桨保持同步。

## Fetch, Decode, Execute, Store

执行一条指令包括一个特定的事件周期。提取，解码，执行和存储。例如，要在 CPU 上方执行添加指令，必须

- 提取：将指令从内存中获取到处理器中。

- 解码：内部解码它要做的事情（在本例中为 add）。

- 执行：从寄存器中获取值，然后将它们实际相加

- 存储：将结果存储回另一个寄存器（Retiring the instruction）。

### CPU 内部结构

在内部，CPU 具有执行上述每个步骤的许多不同子组件，通常它们可以彼此独立工作。这类似于物理生产线，那里有许多工作站，每个步骤都有特定的任务要执行。完成后，它可以将结果传递到下一个测站，并接受新的输入进行处理。

![CPU Insides](https://s2.ax1x.com/2020/01/26/1nkm8J.md.png)

您可以看到指令进入并被处理器解码。CPU 有两种主要类型的寄存器，用于整数计算的寄存器和用于浮点计算的寄存器。浮点数是一种以二进制形式用小数位表示数字的方式，并且在 CPU 中的处理方式有所不同。MMX（多媒体扩展）和 SSE（流单指令多数据）或 Altivec 寄存器类似于浮点寄存器。

寄存器文件是 CPU 内部寄存器的统称。在此之下，我们拥有真正完成所有工作的 CPU 部分。我们说过，处理器要么将一个值加载或存储到寄存器中，要么从一个寄存器加载到内存中，或者对寄存器中的值进行某些操作。

算术逻辑单元（Arithmetic Logic Unit, ALU）是 CPU 操作的核心。它获取寄存器中的值并执行 CPU 能够执行的多种操作。所有现代处理器都有许多 ALU，因此每个都可以独立工作。实际上，奔腾等处理器同时具有快速和慢速 ALU。快速的 ALU 较小（因此您可以在 CPU 上容纳更多），但只能执行最常见的操作，而慢速的 ALU 可以执行所有操作，但更大。

地址生成单元（Address Generation Unit, AGU）处理与高速缓存和主存储器的对话，以将值获取到寄存器中，以供 ALU 进行操作，并将值从寄存器中获取并返回主存储器。浮点寄存器的概念相同，但其组件使用的术语略有不同。

### Pipeling

正如我们在上面看到的，当 ALU 将寄存器加在一起时，与 AGU 将值完全写回内存完全分开，因此没有理由 CPU 不能同时执行这两个操作。我们的系统中还有多个 ALU，每个 ALU 都可以处理独立的指令。最终，CPU 可能会使用其浮点逻辑来执行一些浮点运算，而整数指令也在运行中。这个过程称为流水线，可以做到这一点的处理器称为超标量架构。所有现代处理器都是超标量的。

另一个比喻可能是将管道想象为填充大理石的软管，除非大理石是 CPU 的指令。理想情况下，您将大理石放在一端，另一端（每个时钟脉冲一个），填满管道。一旦装满，对于每一个弹子（指令），您推入所有其他弹子将移至下一个位置，一个弹子将掉出末端（结果）。

但是，分支指令会对这种模型造成严重破坏，因为它们可能会或可能不会导致执行从另一个地方开始。如果您正在流水线工作，则基本上必须猜测分支将走的路，因此您知道将哪些指令带入管道。相反，如果处理器的预测不正确，则一切正常。相反，如果处理器的预测不正确，则会浪费大量时间，必须清理管道并重新启动。此过程通常称为管道冲洗，类似于必须停止并清空软管中的所有弹珠。

### Reordering

实际上，如果 CPU 是软管，则可以自由排序软管中的弹子，只要它们以与放入它们相同的顺序弹出末端即可。我们将其称为程序顺序，因为这是在计算机程序中给出指令的顺序。

```s
1: r3 = r1 * r2
2: r4 = r2 + r3
3: r7 = r5 * r6
4: r8 = r1 + r7
```

指令 2 需要等待指令 1 完全完成才能开始。这意味着管道在等待计算值时必须停顿。类似地，指令 3 和 4 也依赖于 r7。但是，指令 2 和 3 完全没有依赖性。这意味着它们在完全独立的寄存器上运行。如果我们交换指令 2 和 3，由于处理器可以做有用的工作，而不是等待流水线完成以获得上一条指令的结果，因此可以更好地对流水线进行排序。

但是，在编写非常底层的代码时，某些指令可能需要一些有关操作顺序的安全性。我们称这种需求记忆语义。如果您需要获取语义，这意味着对于此说明，您必须确保所有先前说明的结果均已完成。如果您需要发布语义，则是说此之后的所有指令都必须查看当前结果。另一个更为严格的语义是内存屏障或内存屏障，它要求操作在继续之前已提交给内存。

在某些体系结构上，处理器可以为您保证这些语义，而在另一些体系结构上，则必须明确指定它们。尽管您可能会看到这些术语，但大多数程序员无需直接担心它们。

## CISC v RISC

划分计算机体系结构的常见方法是复杂指令集计算机（CISC）和精简指令集计算机（RISC）。在第一个示例中，我们已将值显式加载到寄存器中，执行了加法运算并将保存在另一个寄存器中的结果值存储回内存。这是 RISC 计算方法的示例-仅对寄存器中的值执行运算，并显式地将值加载到存储器中或从存储器中存储值。

CISC 方法可能只是一条指令，该指令从内存中获取值，在内部执行加法并将结果写回。这意味着指令可能需要花费很多时间，但是最终两种方法都达到了相同的目标。所有现代架构都可以看做 RISC 架构：

- 尽管 RISC 使汇编编程变得更加复杂，但是由于几乎所有程序员都使用高级语言，而将汇编代码的生成工作留给了编译器，因此其他优点胜过了这个缺点。

- 因为 RISC 处理器中的指令要简单得多，所以芯片内部有更多的寄存器空间。从内存层次结构中我们知道，寄存器是最快的内存类型，最终所有指令都必须对寄存器中保存的值执行，因此在其他条件相同的情况下，更多的寄存器将导致更高的性能。

- 由于所有指令都在同一时间执行，因此可以进行流水线操作。我们知道流水线化要求将指令流不断地输入到处理器中，因此，如果某些指令花费很长时间而另一些指令却不需要，流水线就变得很复杂，无法有效执行。

### EPIC

在本书的许多示例中都使用过的 Itanium 处理器是经过修改的架构的示例，该架构称为“显式并行指令计算”。我们已经讨论了超标度处理器如何具有在处理器的不同部分中同时运行着许多指令的流水线。显然，为使此功能正常工作，应按照可以充分利用 CPU 可用元素的顺序为处理器提供可能的指令。

传统上组织进入的指令流是硬件的工作。程序按顺序发布指令；处理器必须向前看，并尝试做出有关如何组织传入指令的决定。EPIC 背后的理论是，在更高级别上有更多可用信息，这些信息可以使这些决策比处理器更好。像当前处理器一样，分析汇编语言指令流会丢失程序员可能在原始源代码中提供的许多信息。可以将其视为研究莎士比亚戏剧和阅读相同剧本的 Cliff's Notes 版本之间的区别。两者都能为您提供相同的结果，但是原始图像具有各种额外的信息，可以设置场景并深入了解角色。

因此，订购指令的逻辑可以从处理器转移到编译器。这意味着编译器编写者需要更聪明地尝试为处理器找到最佳的代码顺序。由于处理器的许多工作已移交给编译器，因此处理器也得到了显着简化。

# 内存

## 内存层级

CPU 仅可以直接从位于处理器芯片上的高速缓存中直接获取指令和数据。必须从主系统内存（随机存取内存或 RAM）中加载缓存。但是，RAM 仅在通电时才保留其内容，因此需要存储在更永久的存储器中。

| Speed   | Memory | Description                                                                                                                                                                                                   |
| ------- | ------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Fastest | Cache  | 高速缓存是实际上嵌入在 CPU 内部的内存。高速缓存内存非常快，通常只需要访问一个周期，但是由于它直接嵌入到 CPU 中，因此内存大小是有限制的。实际上，高速缓存有几个子级别（称为 L1，L2，L3），它们的速度都略有提高 |
|         | RAM    | 处理器的所有指令和存储地址都必须来自 RAM。尽管 RAM 速度非常快，但是 CPU 仍需要花费大量时间来访问它（称为*latency*）。RAM 存储在与主板相连的单独的专用芯片中，这意味着它比高速缓存大得多                       |
| Slowest | Disk   | 我们都熟悉软盘或 CDROM 上的软件，并将文件保存到硬盘上。我们还熟悉程序从硬盘加载所需的长时间-具有诸如旋转磁盘和移动磁头之类的物理机制意味着磁盘是最慢的存储形式。但它们也是迄今为止最大的存储形式              |

了解内存层次结构的重点是速度和大小之间的权衡－内存越快，内存越小：

- 空间局部性（Spatial locality）表明，块内的数据可能会一起访问。
- 时间位置（Temporal locality）表明最近使用的数据可能很快会再次使用。

这意味着，通过在实际中尽可能快地实现存储相关信息（空间）小块的快速可访问存储器（时间），可以获得好处。

## 缓存详解

缓存是 CPU 体系结构中最重要的元素之一。要编写高效的代码，开发人员需要了解其系统中的缓存如何工作。高速缓存是较慢的主系统内存的非常快的副本。高速缓存比主存储器要小得多，因为它与寄存器和处理器逻辑一起包含在处理器芯片内。从计算的角度来看，这是主要的房地产，并且其最大大小在经济和物理上都有限制。随着制造商发现越来越多的方法将越来越多的晶体管填充到芯片中，高速缓存的大小已大大增加，但是即使最大的高速缓存也只有几十兆字节，而不是主存储器的千兆字节或硬盘的数兆字节。

缓存由镜像主内存的小块组成。这些块的大小称为行大小，通常为 32 或 64 字节。在谈论缓存时，谈论行大小或缓存行是很常见的事，它指的是镜像主内存的一块。高速缓存只能以高速缓存行的倍数加载和存储内存。缓存具有自己的层次结构，通常称为 L1，L2 和 L3。L1 高速缓存是最快和最小的； L2 更大，更慢，L3 更大，更慢。

L1 缓存通常进一步分为指令缓存和数据，在引入中继的基于哈佛 Mark-1 的计算机之后被称为“哈佛架构”。拆分缓存有助于减少流水线瓶颈，因为较早的流水线阶段倾向于引用指令缓存，而较后的阶段则倾向于数据缓存。除了减少对共享资源的争用之外，为指令提供单独的缓存还允许使用指令流性质的替代实现。它们是只读的，因此不需要昂贵的片上功能（例如多端口），也不需要处理子块读取操作，因为指令流通常使用更常规大小的访问。

![Cache Associativity](https://s2.ax1x.com/2020/01/27/1nssG6.png)

在正常操作期间，处理器会不断要求高速缓存检查高速缓存中是否存储了特定的地址，因此高速缓存需要某种方法来非常快速地查找其是否存在有效行。如果可以将给定地址缓存在缓存中的任何位置，则每次进行引用以确定命中或未命中时都需要搜索每个缓存行。为了保持快速搜索，这是在高速缓存硬件中并行完成的，但是对于合理大小的高速缓存而言，搜索每个条目通常过于昂贵。因此，可以通过限制特定地址必须驻留的位置来简化缓存。这是一个权衡； 高速缓存显然比系统内存小得多，因此某些地址必须别名。如果两个彼此互为别名的地址一直在不断更新，则它们将争用缓存行。

- 直接映射的缓存（Direct mapped caches）将允许缓存行仅存在于缓存中的单个条目中。这是最简单的在硬件中实现的方法，由于两个阴影地址必须共享同一条缓存行，因此无法避免混淆现象。

- 完全关联高速缓存（Fully Associative caches）将允许高速缓存行存在于高速缓存的任何条目中。由于可以使用任何条目，因此可以避免别名问题。但是在硬件中实现非常昂贵，因为必须同时查找每个可能的位置以确定值是否在缓存中。

- 集合关联缓存（Set Associative caches）是直接关联和完全关联缓存的混合，并且允许特定的缓存值存在于缓存内的某些行子集中。高速缓存被划分为偶数部分，称为方式，并且可以以任何方式定位特定地址。因此，n 路集关联缓存将允许在行大小为 set n 的总块 mod n 中的任何条目中存在一条缓存行。上图中“缓存关联性”显示了一个示例的 8 元素，4 路集关联缓存。在这种情况下，这两个地址具有四个可能的位置，这意味着在查找时仅必须搜索一半的缓存。方式越多，可能的位置就越多，混淆现象就越少，从而导致总体上更好的性能。

一旦缓存已满，处理器就需要清除一行来为新行腾出空间，处理器可以通过多种算法选择逐出哪条线。例如，最近最少使用（LRU）是一种算法，其中丢弃最旧的未使用的行以为新行腾出空间。当仅从高速缓存中读取数据时，无需确保与主内存的一致性。但是，当处理器开始写入高速缓存行时，它需要就如何更新底层主内存做出一些决定。

- 直写式高速缓存（Write-through）将在处理器更新高速缓存时将更改直接写入主系统内存。这是较慢的，因为如我们所见，写入主存储器的过程较慢。

- 回写缓存（Write-back）会将更改延迟写入 RAM，直到绝对必要为止。明显的优点是，写入缓存条目时需要较少的主存储器访问。已写入但未提交给内存的缓存行称为脏行。缺点是，当退出缓存条目时，它可能需要两次内存访问（一次写入脏数据主内存，另一次加载新数据）。

如果一个条目同时存在于较高级别和较低级别的高速缓存中，则我们说较高级别的高速缓存是 Inclusive；否则，如果具有一行的较高级别的高速缓存消除了具有该行的较低级别的高速缓存的可能性，我们说它是排他的（Exclusive）。

## 缓存寻址

到目前为止，我们还没有讨论过缓存如何确定给定地址是否驻留在缓存中。显然，高速缓存必须保留当前驻留在高速缓存行中的数据的目录。缓存目录和数据可能位于同一处理器上，但也可能是分开的，例如在具有核心 L3 目录的 POWER5 处理器的情况下，但是实际上访问数据需要遍历 L3 总线才能访问 核心内存。这样的安排可以促进更快的命中/未命中处理，而不会产生将整个缓存保留在内核中的其他成本。

![Cache tags](https://s2.ax1x.com/2020/01/27/1n67Dg.png)

需要并行检查标签以降低等待时间。更多的标记位（即，较少的设置关联性）需要更复杂的硬件来实现。另外，更多的集合关联性意味着更少的标签，但是处理器现在需要硬件来多路复用许多集合的输出，这也可能增加延迟。

为了快速确定地址是否位于缓存中，将其分为三个部分：标签，索引和偏移量。偏移位取决于高速缓存的行大小。例如，一个 32 字节的行大小将使用地址的最后 5 位（即 25）作为行的偏移量。索引是一个条目可能驻留的特定缓存行。例如，让我们考虑一个具有 256 个条目的缓存。如果这是一个直接映射的缓存，我们知道数据可能仅驻留在一条可能的行中，因此偏移量后的下一个 8 位（28）描述了要检查的行-0 到 255 之间。

现在，考虑相同的 256 元素高速缓存，但是分为两种方式。这意味着有两组 128 行，并且给定地址可以位于这两个组中的任何一个中。因此，仅需要 7 位作为索引即可偏移到 128 个条目的路径中。对于给定的高速缓存大小，随着方法数量的增加，由于每种方法都会变小，因此减少了作为索引所需的位数。高速缓存目录仍然需要检查高速缓存中存储的特定地址是否是它感兴趣的那个地址。因此，该地址的其余位是高速缓存目录对照传入的地址标记位进行检查以确定是否存在标记位。缓存命中与否。“缓存标签”中说明了这种关系。

当存在多种方式时，此检查必须在每种方式中并行进行，然后将其结果传递到多路复用器，该多路复用器输出最终的命中或未命中结果。如上所述，高速缓存的关联性越高，索引所需的位越少，而标记位则越多-到完全关联的高速缓存的极端（其中没有位用作索引位）。标签位的并行匹配是高速缓存设计的昂贵组件，并且通常是高速缓存可以增长多少行（即，多大）的限制因素。

# 外设与总线（Peripherals and buses）

外设（Peripherals）是连接到计算机的许多外部设备中的任何一个。显然，处理器必须通过某种方式与外围设备对话，以使其变得有用。处理器与外围设备之间的通信通道称为总线。设备需要输入和输出才有用，与外设进行有效通信需要许多通用概念。

## 中断（Interrupts）

中断允许设备从字面上中断处理器以标记某些信息。例如，当按键被按下时，将产生中断，以将按键事件传递给操作系统。通过操作系统和 BIOS 的某种组合为每个设备分配了一个中断。设备通常连接到可编程中断控制器（PIC），可编程中断控制器是主板的一部分，用于缓冲中断信息并将其传达给主处理器。每个设备之间都有一条物理中断线，该中断线是系统提供的 PIC 之一。当设备想要中断时，它将修改该线上的电压。

PIC 角色的一个非常广泛的描述是它接收到该中断并将其转换为一条消息，供主处理器使用。尽管具体过程随体系结构而变化，但一般原则是操作系统已配置了一个中断描述符表，该表将每个可能的中断与代码地址配对，以在接收到中断时跳转到该地址。编写此中断处理程序是设备驱动程序作者与操作系统结合的工作，设备将中断引发给中断控制器，后者将信息传递到处理器；处理器查看由操作系统填写的描述符表，以找到处理故障的代码。

![Overview of handling an interrupt](https://s2.ax1x.com/2020/01/27/1n2Ac6.png)

大多数驱动程序会将中断的处理分为下半部分和上半部分。下半部分将确认中断，将处理操作排队，并使处理器返回到其正在快速执行的操作。然后，当 CPU 空闲时，上半部分将稍后运行，并进行更密集的处理。这是为了停止占用整个 CPU 的中断。

### 保存状态

由于中断随时可能发生，因此在完成中断处理后，您必须返回正在运行的操作，这一点很重要。确保进入中断处理程序后，它保存任何状态通常是操作系统的工作。即寄存器，并从中断处理程序返回时将其恢复。这样，除了浪费一些时间外，中断对于当时正在运行的任何事物都是完全透明的。

### Interrupts vs traps and exceptions

虽然中断通常与物理设备的外部事件相关联，但相同的机制对于处理内部系统操作很有用。例如，如果处理器检测到条件，例如对无效内存的访问，除以零的尝试或无效指令，则它可以在内部引发要由操作系统处理的异常。它也是用于捕获操作系统中用于系统调用的机制以及实现虚拟内存的机制。尽管内部生成而不是外部生成，但是异步中断正在运行的代码的原理仍然相同。

### 中断的类型

在线路上发出中断信号的主要方式有两种：电平触发和边沿触发。电平触发的中断定义了中断线的电压保持高电平，以指示中断待处理。边沿触发的中断检测总线上的转换；即线电压从低变高。对于边沿触发的中断，PIC 会检测到方波脉冲作为信号并发出中断。当设备共享中断线时，差异显着。在电平触发的系统中，中断线将为高电平，直到处理了引发中断的所有设备并取消声明其中断为止。在边沿触发系统中，线路上的脉冲将向 PIC 指示已发生中断，并将其发信号通知操作系统进行处理。但是，如果另外的脉冲从另一个设备进入已经确定的线路。

电平触发的中断的问题在于，可能需要花费大量时间来处理设备的中断。在这段时间内，中断线保持高电平，无法确定是否有其他设备在该线上引发了中断。这意味着在服务中断时可能会有相当长的不可预测的延迟。使用边沿触发的中断，可以注意到长时间运行的中断并在其中排队，但是在发生这种情况时，共享该线路的其他设备仍可以转换（并因此引发中断）。但是，这带来了新的问题。如果两个设备同时中断，则可能会错过其中一个中断，或者环境或其他干扰可能会产生虚假中断，应将其忽略。

### 不可屏蔽的中断

对于系统而言，一定时间掩盖或防止中断很重要。通常，可以将中断置于保留状态，但是特定类别的中断（称为不可屏蔽中断（NMI））是该规则的例外。典型示例是复位中断。NMI 对于实现诸如系统监视程序之类的事情很有用，在该系统中，NMI 会定期提高并设置一些必须由操作系统确认的标志。如果在下一个定期 NMI 之前未看到该确认，则可以认为系统没有取得进展。另一个常见用法是对系统进行概要分析。可以提出定期的 NMI 并将其用于评估处理器当前正在运行的代码。随着时间的流逝，这将建立一个正在运行的代码的配置文件，并对系统性能产生非常有用的洞察力。

## IO Space 与 DMA

显然，处理器将需要与外围设备进行通信，并通过 IO 操作进行通信。IO 的最常见形式是所谓的内存映射 IO，其中设备上的寄存器映射到内存中。这意味着要与设备通信，您只需简单地读取或写入内存中的特定地址即可，这也就是 IO Space 的概念。

由于设备的速度远远低于处理器的速度，因此需要采取某种方法来避免 CPU 等待来自设备的数据。直接内存访问（DMA）是一种在外围设备和系统 RAM 之间直接传输数据的方法。驱动程序可以通过为设备设置 RAM 区域以放置其数据来进行 DMA 传输。然后，它可以开始 DMA 传输，并允许 CPU 继续执行其他任务。一旦设备完成，它将引发一个中断并向驱动程序发送信号，表明传输已完成。从那时起，设备中的数据（例如磁盘中的文件或视频捕获卡中的帧）就已存储在内存中，可以使用了。

## 其他总线

其他总线连接在 PCI 总线和外部设备之间。从操作系统的角度来看，USB 设备是一组端点，这些端点组合在一起成为一个接口端点可以是入站或出站，因此只能在一个方向上传输数据端点可以具有许多不同的类型：

- 控制端点用于配置设备等。

- 中断端点用于传输少量数据，他们比其他的端点具有更高的优先级。

- 批量端点，这些端点传输大量数据，但没有保证的时间限制。

- 同步传输是高优先级的实时传输，但是如果错过它们，则不会重试这用于流式传输视频或音频之类的数据，而毫无意义地再次发送数据。

可以有许多接口（由多个端点组成），并且接口被分组为配置但是，大多数设备只有一个配置。

![An overview of a UCHI controller](https://s2.ax1x.com/2020/01/27/1n2rvV.md.png)

上图显示了通用主机控制器接口或 UHCI 的概述。它概述了如何通过硬件和软件的组合将 USB 数据移出系统。本质上，该软件以指定的格式设置数据模板，以供主机控制器读取和通过 USB 总线发送。从概述的左上角开始，控制器具有一个帧寄存器，该寄存器带有一个计数器，该计数器周期性地（每毫秒）递增。该值用于索引由软件创建的框架列表。该表中的每个条目都指向传输描述符队列。软件会将该数据设置在内存中，然后由驱动 USB 总线的独立芯片即主机控制器读取。软件需要安排工作队列，以便将 90％的帧时间分配给同步数据，剩下 10％的时间用于中断，控制和批量数据。

从图中可以看出，数据的链接方式意味着等时数据的传输描述符仅与一个特定的帧指针关联（换句话说，仅与一个特定的时间段关联），之后将被丢弃。但是，中断，控制和批量数据都在同步数据之后排队，因此，如果不在一个帧（时间段）内发送，则将在下一帧进行。USB 层通过 USB 请求块或 URB 进行通信 URB 包含有关此请求与哪个端点有关的信息，数据，任何相关信息或属性以及当 URB 完成时要调用的回调函数 USB 驱动程序将固定格式的 URB 提交给 USB 内核，后者与上述 USB 主机控制器协同管理它们 USB 内核会将您的数据发送到 USB 设备，并在完成后触发您的回叫。
